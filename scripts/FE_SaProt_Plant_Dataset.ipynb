{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba910c0c-a683-47be-8177-4876bc57351d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from Bio import SeqIO\n",
    "from transformers import T5Tokenizer, T5EncoderModel\n",
    "import torch\n",
    "import re\n",
    "from io import StringIO\n",
    "from Bio import SeqIO\n",
    "import os\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "from IPython.display import clear_output\n",
    "import sys\n",
    "import os\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f67f7d8-66cb-4177-b75b-65364537e56c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# pLMs\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\"\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "586440d9-002d-4692-a8f2-6df825604de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ProstT5():\n",
    "    global prostt5_tokenizer, prostt5_model\n",
    "    if \"prostt5_tokenizer\" not in globals():\n",
    "        prostt5_tokenizer = T5Tokenizer.from_pretrained('Rostlab/ProstT5_fp16')\n",
    "    if \"prostt5_model\" not in globals():\n",
    "        prostt5_model = T5EncoderModel.from_pretrained(\"Rostlab/ProstT5_fp16\").to(device).eval()\n",
    "\n",
    "\n",
    "\n",
    "# Load the CNN model once and make it a global variable\n",
    "class CNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1024, 32, kernel_size=(7, 1), padding=(3, 0)),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.0),\n",
    "            torch.nn.Conv2d(32, 20, kernel_size=(7, 1), padding=(3, 0))\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1).unsqueeze(dim=-1)\n",
    "        Yhat = self.classifier(x)\n",
    "        Yhat = Yhat.squeeze(dim=-1)\n",
    "        return Yhat\n",
    "\n",
    "# Function to predict 3Di sequence\n",
    "def predict_3Di(sequence):\n",
    "    \"\"\"\n",
    "    Predict 3Di sequence from an amino acid sequence.\n",
    "\n",
    "    Args:\n",
    "        sequence (str): Amino acid sequence.\n",
    "\n",
    "    Returns:\n",
    "        str: Predicted 3Di sequence.get_ProtT5_embeddings(accession, Seq_AA, site, feature_folder)\n",
    "    \"\"\"\n",
    "    global prostt5_model, prostt5_tokenizer, cnn_model\n",
    "\n",
    "    # Preprocess the sequence\n",
    "    prefix = \"<AA2fold>\"\n",
    "    seq = prefix + ' ' + ' '.join(list(sequence))\n",
    "    token_encoding = prostt5_tokenizer(seq, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Generate embeddings using the T5 model\n",
    "    with torch.no_grad():\n",
    "        embedding_repr = prostt5_model(**token_encoding)\n",
    "        embedding = embedding_repr.last_hidden_state[:, 1:, :]  # Skip special token\n",
    "        prediction = cnn_model(embedding)\n",
    "        prediction = prediction.argmax(dim=1).squeeze().cpu().numpy()\n",
    "\n",
    "    # Map predictions to 3Di symbols\n",
    "    ss_mapping = {\n",
    "        0: \"A\", 1: \"C\", 2: \"D\", 3: \"E\", 4: \"F\", 5: \"G\", 6: \"H\", 7: \"I\",\n",
    "        8: \"K\", 9: \"L\", 10: \"M\", 11: \"N\", 12: \"P\", 13: \"Q\", 14: \"R\", 15: \"S\",\n",
    "        16: \"T\", 17: \"V\", 18: \"W\", 19: \"Y\"\n",
    "    }\n",
    "    predicted_3Di = \"\".join([ss_mapping[p] for p in prediction])\n",
    "    return predicted_3Di.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44ca7b70-9833-4c15-8271-10bdbed7035d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_SaProt_embeddings(Seq_AA):\n",
    "    \"\"\"\n",
    "    Get or compute SaProt embeddings for a protein sequence and its structural information.\n",
    "\n",
    "    Parameters:\n",
    "    - accession (str): Accession ID of the protein.\n",
    "    - Seq_AA (str): Amino acid sequence of the protein.\n",
    "    - site (int): Position of interest in the sequence.\n",
    "    - feature_folder (str): Path to the folder containing precomputed features.\n",
    "    - saprot_tokenizer: Tokenizer for SaProt.\n",
    "    - saprot_model: Model for generating embeddings.\n",
    "    - device: PyTorch device (e.g., 'cpu' or 'cuda').\n",
    "\n",
    "    Returns:\n",
    "    - torch.Tensor: Averaged representation of protein sequence.\n",
    "    \"\"\"\n",
    "\n",
    "    Seq_3Di = predict_3Di(Seq_AA) # Use the provided foldseek code if pdb available\n",
    "    \n",
    "    # Combine sequence and structure\n",
    "    combined_AA_3Di = \"\".join([a + b for a, b in zip(Seq_AA, Seq_3Di)])\n",
    "    \n",
    "    # Tokenize sequence\n",
    "    inputs = saprot_tokenizer(combined_AA_3Di, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}  # Move inputs to the correct device\n",
    "    \n",
    "    # Generate embeddings\n",
    "    embeddings_per_residue = saprot_model.get_hidden_states(inputs)[0]\n",
    "\n",
    "    # Compute protein-level representation (mean pooling)\n",
    "    protein_representation = embeddings_per_residue.mean(dim=0)\n",
    "    \n",
    "    return protein_representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0eee49ca-7b8a-45af-864d-e662098e53cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_fasta(file_path):\n",
    "    data = []\n",
    "    for record in SeqIO.parse(file_path, \"fasta\"):\n",
    "        accession_parts = record.id.split(\"|\")\n",
    "        accession = accession_parts[1] if len(accession_parts) > 1 else record.id  # Extract second part if exists\n",
    "        AA_Seq = str(record.seq)\n",
    "        data.append((accession, AA_Seq))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90969898-f474-49da-850c-7aabcc425360",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.saprot.base import SaprotBaseModel\n",
    "from transformers import EsmTokenizer\n",
    "\n",
    "def load_SaProt():\n",
    "    global saprot_model, saprot_tokenizer\n",
    "\n",
    "    saprot_config = {\n",
    "        \"task\": \"base\",\n",
    "        \"config_path\": \"model/saprot/SaProt_650M_AF2/\", # Note this is the directory path of SaProt, not the \".pt\" file\n",
    "        \"load_pretrained\": True,\n",
    "    }\n",
    "    \n",
    "    if \"saprot_tokenizer\" not in globals():\n",
    "        saprot_tokenizer = EsmTokenizer.from_pretrained(saprot_config[\"config_path\"])\n",
    "    if \"saprot_model\" not in globals():\n",
    "        saprot_model = SaprotBaseModel(**saprot_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b37d400-7f65-4591-b98f-fa4cf39eeae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ProstT5\n",
    "load_ProstT5()\n",
    "\n",
    "# Load SaProt\n",
    "load_SaProt()\n",
    "\n",
    "# Load CNN model\n",
    "cnn_model = CNN()\n",
    "checkpoint_path_3Di_prediction = \"AA_to_3Di_prostt5_cnn_model.pt\"\n",
    "state = torch.load(checkpoint_path_3Di_prediction, map_location=device)\n",
    "cnn_model.load_state_dict(state[\"state_dict\"])\n",
    "cnn_model = cnn_model.to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cddb9c9d-96f9-4bed-8ad2-9790247a8f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "SaProt_features = get_SaProt_embeddings(\"MHWIATRNAVVSFPKWRFFFRSSYRTYSSLKPSSPILLNRRYSEGISCLRDGKSLKRITTASKKVKTSSDVLTDKDLSHLVWWKERLQTCKKPSTLQLIERLMYTNLLGLDPSLRNGSLKDGNLNWEMLQFKSRFPREVLLCRVGEFYEAIGIDACILVEYAGLNPFGGLRSDSIPKAGCPIMNLRQTLDDLTRNGYSVCIVEEVQGPTPARSRKGRFISGHAHPGSPYVYGLVGVDHDLDFPDPMPVVGISRSARGYCMISIFETMKAYSLDDGLTEEALVTKLRTRRCHHLFLHASLRHNASGTCRWGEFGEGGLLWGECSSRNFEWFEGDTLSELLSRVKDVYGLDDEVSFRNVNVPSKNRPRPLHLGTATQIGALPTEGIPCLLKVLLPSTCSGLPSLYVRDLLLNPPAYDIALKIQETCKLMSTVTCSIPEFTCVSSAKLVKLLEQREANYIEFCRIKNVLDDVLHMHRHAELVEILKLLMDPTWVATGLKIDFDTFVNECHWASDTIGEMISLDENESHQNVSKCDNVPNEFFYDMESSWRGRVKGIHIEEEITQVEKSAEALSLAVAEDFHPIISRIKATTASLGGPKGEIAYAREHESVWFKGKRFTPSIWAGTAGEDQIKQLKPALDSKGKKVGEEWFTTPKVEIALVRYHEASENAKARVLELLRELSVKLQTKINVLVFASMLLVISKALFSHACEGRRRKWVFPTLVGFSLDEGAKPLDGASRMKLTGLSPYWFDVSSGTAVHNTVDMQSLFLLTGPNGGGKSSLLRSICAAALLGISGLMVPAESACIPHFDSIMLHMKSYDSPVDGKSSFQVEMSEIRSIVSQATSRSLVLIDEICRGTETAKGTCIAGSVVESLDTSGCLGIVSTHLHGIFSLPLTAKNITYKAMGAENVEGQTKPTWKLTDGVCRESLAFETAKREGVPESVIQRAEALYLSVYAKDASAEVVKPDQIITSSNNDQQIQKPVSSERSLEKDLAKAIVKICGKKMIEPEAIECLSIGARELPPPSTVGSSCVYVMRRPDKRLYIGQTDDLEGRIRAHRAKEGLQGSSFLYLMVQGKSMACQLETLLINQLHEQGYSLANLADGKHRNFGTSSSLSTSDVVSIL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4e5f768-76cd-4e07-a6de-d3f58fab8c82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1280])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SaProt_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f4caaac1-2fda-49d1-b158-1f09ebbd77a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define directory and file paths\n",
    "data_dir = \"/home/sp2530/Desktop/DNA-Binding-V2/data/plant/fasta/\"\n",
    "\n",
    "file_paths = [\n",
    "    \"DBP_independent.fasta\",\n",
    "    \"DBP.fasta\",\n",
    "    \"non_DBP.fasta\",\n",
    "    \"non_DBP_independent.fasta\"\n",
    "]\n",
    "\n",
    "# Parse FASTA files with full paths in a single list comprehension\n",
    "data = [entry for file_path in file_paths for entry in parse_fasta(data_dir + file_path)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "89fc4e37-ba3f-4445-b31a-157350aca0c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Q5Z807',\n",
       "  'MSRRQEICRNFQRGSCKYGAQCRYLHASPHQQQQQQQAKPNPFGFGTGSRQQQQPSFGSQFQQQQQQQQKPNPFGFGVQGANAQSRNAPGPAKPFQNKWVRDPSAPTKQTEAVQPPQAQAAHTSCEDPQSCRQQISEDFKNEAPIWKLTCYAHLRNGPCNIKGDISFEELRAKAYEEGKQGHSLQSIVEGERNLQNAKLMEFTNLLNSARPSQTPSFPTMSSFPEVKNNSSFGASQTNGPPVFSSFSQIGAATNIGPGPGTTAPGMPASSPFGHPSSAPLAAPTFGSSQMKFGVSSVFGNQGSGQPFGSFQAPRFPSSKSPASSVQHRDIDRQSQELLNGMVTPPSVMFEESVGNNKNENQDDSIWLKEKWAIGEIPLDEPPQRHVSHVF'),\n",
       " ('C0SVV6',\n",
       "  'MRIPTYDFGSKFSVVQEVMRLQTVKHFLEPVLEPLIRKVVKEEVELALGKHLAGIKWICEKETHPLESRNLQLKFLNNLSLPVFTSARIEGDEGQAIRVGLIDPSTGQIFSSGPASSAKLEVFVVEGDFNSVSDWTDEDIRNNIVREREGKKPLLNGNVFAVLNDGIGVMDEISFTDNSSWTRSRKFRLGVRIVDQFDYVKIREAITESFVVRDHRGELYKKHHPPSLFDEVWRLEKIGKDGAFHRRLNLSNINTVKDFLTHFHLNSSKLRQVLGTGMSSKMWEITLDHARSCVLDSSVHVYQAPGFQKKTAVVFNVVAQVLGLLVDFQYIPAEKLSEIEKAQAEVMVIDALSHLNEVISYDDEVSMMRNVLNAPASQGSVAGIDYSGLSLTSLDGYGFVSSLHNTAECSGKHSDDVDMEVTPHGLYEDYDNLWNCSHILGLEEPQSELQSALDDFMSQKNASVGGKAHSKRWTKLFSVSRWLSVFKYVKLGKI')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "37a07ca3-6c1c-4a8d-a5a4-6fbcfb705c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "saprot_features_disk = \"/home/sp2530/Desktop/DNA-Binding-V2/data/plant/features/saprot\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "40784fb0-6af9-45df-b557-1a991bc70755",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C0SVV6',\n",
       " 'Q5Z807',\n",
       " 'Q75LX7',\n",
       " 'Q7XC57',\n",
       " 'Q84JF0',\n",
       " 'Q8LFK2',\n",
       " 'Q9FVV7',\n",
       " 'Q9FX84'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Create already_extracted_set\n",
    "already_extracted_set = {\n",
    "    os.path.splitext(file)[0].replace(\"_saprot\", \"\")  # extract accession from file name\n",
    "    for file in os.listdir(saprot_features_disk)\n",
    "    if file.endswith(\"_saprot.pt\")\n",
    "}\n",
    "\n",
    "already_extracted_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19c7db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                  | 0/2695 [00:00<?, ?it/s]/tmp/ipykernel_1092710/181073214.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  embeddings_per_residue = torch.load(feature_file)\n",
      "100%|████████████████████████████████████▉| 2693/2695 [2:28:48<00:09,  4.71s/it]"
     ]
    }
   ],
   "source": [
    "# Feature extraction\n",
    "for accession, Seq_AA in tqdm.tqdm(data):\n",
    "    get_SaProt_embeddings(accession, Seq_AA, saprot_features_disk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7124ee-09c8-4240-8f08-41e1208b504c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
