{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6062633c-12d8-47eb-9855-3d16c955aff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-17 23:54:53.151826: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-03-17 23:54:53.238454: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-03-17 23:54:53.262130: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-17 23:54:53.399003: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-03-17 23:54:54.722564: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from Bio import SeqIO\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import T5Tokenizer, T5EncoderModel\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a8278c-b17a-4895-a58d-e542122b1822",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31e40f1f-70a7-496e-b75e-031134ca8215",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAIN_DIR = \"/home/sp2530/Desktop/DNA-Binding-Protein-Prediction/standalone_version\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1accdb53-9f57-4e11-b149-46e5f3ddb4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load models\n",
    "ProtT5_ann_model = load_model(os.path.join(MAIN_DIR, \"models/ProtT5_pLMDBPs.keras\"), compile=False)\n",
    "SaProt_ann_model = load_model(os.path.join(MAIN_DIR, \"models/SaProt_pLMDBPs.keras\"), compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a634edd-c4fc-46c7-a074-af08c966c4b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-17 22:49:34.804312: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13514 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060 Ti, pci bus id: 0000:02:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "tf_device = tf.device('cpu')\n",
    "torch_device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efcdc78-957b-4e2a-a6ae-11a8ff3de271",
   "metadata": {},
   "source": [
    "### ProtT5 PLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b341c4cf-397a-4897-9db4-2f9d88b3d865",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_ProtT5():\n",
    "    global prott5_tokenizer, prott5_model\n",
    "    if \"prott5_tokenizer\" not in globals():\n",
    "        prott5_tokenizer = T5Tokenizer.from_pretrained('Rostlab/prot_t5_xl_uniref50')\n",
    "    if \"prott5_model\" not in globals():\n",
    "        prott5_model = T5EncoderModel.from_pretrained('Rostlab/prot_t5_xl_uniref50').eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37da148e-8a18-4766-95e3-f1be0361f846",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ProtT5_embeddings(sequence): \n",
    "    # replace rare amino acids with X\n",
    "    sequence = re.sub(r\"[UZOB]\", \"X\", sequence)\n",
    "    \n",
    "    # Add space in between amino acids\n",
    "    sequence = [ ' '.join(sequence)]\n",
    "    \n",
    "    # set configurations and extract features\n",
    "    ids = prott5_tokenizer.batch_encode_plus(sequence, add_special_tokens=True, padding=True)\n",
    "    input_ids = torch.tensor(ids['input_ids']).to(torch_device)\n",
    "    attention_mask = torch.tensor(ids['attention_mask']).to(torch_device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        embedding = prott5_model(input_ids=input_ids,attention_mask=attention_mask)\n",
    "    embedding = embedding.last_hidden_state.cpu().numpy()\n",
    "    \n",
    "    # find length\n",
    "    seq_len = (attention_mask[0] == 1).sum()\n",
    "    \n",
    "    # average over diemnsion to geta  single per protein embedding\n",
    "    seq_emd = embedding[0][:seq_len].mean(axis=0) # shape (1024)\n",
    "\n",
    "    return seq_emd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54695ad0-0e7e-4d91-9309-95f5222c58fa",
   "metadata": {},
   "source": [
    "### SaProt PLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "572b54fb-05e0-466f-afc3-c84cfdcaf179",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from Bio import SeqIO\n",
    "from transformers import T5Tokenizer, T5EncoderModel\n",
    "import torch\n",
    "import re\n",
    "from io import StringIO\n",
    "from Bio import SeqIO\n",
    "import os\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "from IPython.display import clear_output\n",
    "import sys\n",
    "import os\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b83cd57-e18f-4865-8b4d-0168e6c3d764",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ProstT5():\n",
    "    global prostt5_tokenizer, prostt5_model\n",
    "    if \"prostt5_tokenizer\" not in globals():\n",
    "        prostt5_tokenizer = T5Tokenizer.from_pretrained('Rostlab/ProstT5_fp16')\n",
    "    if \"prostt5_model\" not in globals():\n",
    "        prostt5_model = T5EncoderModel.from_pretrained(\"Rostlab/ProstT5_fp16\").to(device).eval()\n",
    "\n",
    "\n",
    "\n",
    "# Load the CNN model once and make it a global variable\n",
    "class CNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1024, 32, kernel_size=(7, 1), padding=(3, 0)),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.0),\n",
    "            torch.nn.Conv2d(32, 20, kernel_size=(7, 1), padding=(3, 0))\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1).unsqueeze(dim=-1)\n",
    "        Yhat = self.classifier(x)\n",
    "        Yhat = Yhat.squeeze(dim=-1)\n",
    "        return Yhat\n",
    "\n",
    "# Function to predict 3Di sequence\n",
    "def predict_3Di(sequence):\n",
    "    \"\"\"\n",
    "    Predict 3Di sequence from an amino acid sequence.\n",
    "\n",
    "    Args:\n",
    "        sequence (str): Amino acid sequence.\n",
    "\n",
    "    Returns:\n",
    "        str: Predicted 3Di sequence.get_ProtT5_embeddings(accession, Seq_AA, site, feature_folder)\n",
    "    \"\"\"\n",
    "    global prostt5_model, prostt5_tokenizer, cnn_model\n",
    "\n",
    "    # Preprocess the sequence\n",
    "    prefix = \"<AA2fold>\"\n",
    "    seq = prefix + ' ' + ' '.join(list(sequence))\n",
    "    token_encoding = prostt5_tokenizer(seq, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Generate embeddings using the T5 model\n",
    "    with torch.no_grad():\n",
    "        embedding_repr = prostt5_model(**token_encoding)\n",
    "        embedding = embedding_repr.last_hidden_state[:, 1:, :]  # Skip special token\n",
    "        prediction = cnn_model(embedding)\n",
    "        prediction = prediction.argmax(dim=1).squeeze().cpu().numpy()\n",
    "\n",
    "    # Map predictions to 3Di symbols\n",
    "    ss_mapping = {\n",
    "        0: \"A\", 1: \"C\", 2: \"D\", 3: \"E\", 4: \"F\", 5: \"G\", 6: \"H\", 7: \"I\",\n",
    "        8: \"K\", 9: \"L\", 10: \"M\", 11: \"N\", 12: \"P\", 13: \"Q\", 14: \"R\", 15: \"S\",\n",
    "        16: \"T\", 17: \"V\", 18: \"W\", 19: \"Y\"\n",
    "    }\n",
    "    predicted_3Di = \"\".join([ss_mapping[p] for p in prediction])\n",
    "    return predicted_3Di.lower()\n",
    "\n",
    "def get_SaProt_embeddings(Seq_AA):\n",
    "    \"\"\"\n",
    "    Get or compute SaProt embeddings for a protein sequence and its structural information.\n",
    "\n",
    "    Parameters:\n",
    "    - accession (str): Accession ID of the protein.\n",
    "    - Seq_AA (str): Amino acid sequence of the protein.\n",
    "    - site (int): Position of interest in the sequence.\n",
    "    - feature_folder (str): Path to the folder containing precomputed features.\n",
    "    - saprot_tokenizer: Tokenizer for SaProt.\n",
    "    - saprot_model: Model for generating embeddings.\n",
    "    - device: PyTorch device (e.g., 'cpu' or 'cuda').\n",
    "\n",
    "    Returns:\n",
    "    - torch.Tensor: Averaged representation of protein sequence.\n",
    "    \"\"\"\n",
    "\n",
    "    Seq_3Di = predict_3Di(Seq_AA) # Use the provided foldseek code if pdb available\n",
    "    \n",
    "    # Combine sequence and structure\n",
    "    combined_AA_3Di = \"\".join([a + b for a, b in zip(Seq_AA, Seq_3Di)])\n",
    "    \n",
    "    # Tokenize sequence\n",
    "    inputs = saprot_tokenizer(combined_AA_3Di, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}  # Move inputs to the correct device\n",
    "    \n",
    "    # Generate embeddings\n",
    "    embeddings_per_residue = saprot_model.get_hidden_states(inputs)[0]\n",
    "\n",
    "    # Compute protein-level representation (mean pooling)\n",
    "    protein_representation = embeddings_per_residue.mean(dim=0)\n",
    "    \n",
    "    return protein_representation\n",
    "\n",
    "\n",
    "from model.saprot.base import SaprotBaseModel\n",
    "from transformers import EsmTokenizer\n",
    "\n",
    "def load_SaProt():\n",
    "    global saprot_model, saprot_tokenizer\n",
    "\n",
    "    saprot_config = {\n",
    "        \"task\": \"base\",\n",
    "        \"config_path\": \"model/saprot/SaProt_650M_AF2/\", # Note this is the directory path of SaProt, not the \".pt\" file\n",
    "        \"load_pretrained\": True,\n",
    "    }\n",
    "    \n",
    "    if \"saprot_tokenizer\" not in globals():\n",
    "        saprot_tokenizer = EsmTokenizer.from_pretrained(saprot_config[\"config_path\"])\n",
    "    if \"saprot_model\" not in globals():\n",
    "        saprot_model = SaprotBaseModel(**saprot_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3aca9fc2-5d2a-4dbb-ab38-a77726ab14e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load required models\n",
    "\n",
    "# Load ProtT5\n",
    "load_ProtT5\n",
    "\n",
    "# Load ProstT5\n",
    "load_ProstT5()\n",
    "\n",
    "# Load SaProt\n",
    "load_SaProt()\n",
    "\n",
    "# Load CNN model\n",
    "cnn_model = CNN()\n",
    "checkpoint_path_3Di_prediction = \"AA_to_3Di_prostt5_cnn_model.pt\"\n",
    "state = torch.load(checkpoint_path_3Di_prediction, map_location=device)\n",
    "cnn_model.load_state_dict(state[\"state_dict\"])\n",
    "cnn_model = cnn_model.to(device).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529f2772-998d-45e8-8f46-e8ce29605dcc",
   "metadata": {},
   "source": [
    "### Read Input Fasta File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435da9ff-a91d-4a9f-a28d-0979e6289e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_fasta(file_path):\n",
    "    results = []\n",
    "\n",
    "    for record in SeqIO.parse(file_path, \"fasta\"):\n",
    "        description = record.id\n",
    "        \n",
    "        # Amino Acid Sequence\n",
    "        AA_Seq = str(record.seq)\n",
    "\n",
    "        # Get pLM representations\n",
    "        ProtT5_embeddings = get_ProtT5_embeddings(AA_Seq)\n",
    "        SaProt_embeddings = get_SaProt_embeddings(AA_Seq)\n",
    "\n",
    "        # Get predictions\n",
    "        ProtT5_ANN_prob = ProtT5_ann_model(ProtT5_embeddings).item()\n",
    "        SaProt_ANN_prob = SaProt_ann_model(SaProt_embeddings).item()\n",
    "\n",
    "        # Average prediction\n",
    "        avg_prob = (ProtT5_ANN_prob + SaProt_ANN_prob) / 2\n",
    "\n",
    "        # Final prediction (binary)\n",
    "        final_prediction = avg_prob > 0.5\n",
    "\n",
    "        # Store results\n",
    "        results.append({\n",
    "            \"description\": description,\n",
    "            \"ProtT5_ANN_prob\": ProtT5_ANN_prob,\n",
    "            \"SaProt_ANN_prob\": SaProt_ANN_prob,\n",
    "            \"avg_prob\": avg_prob,\n",
    "            \"final_prediction\": final_prediction\n",
    "        })\n",
    "\n",
    "    # Convert results to DataFrame\n",
    "    df = pd.DataFrame(results)\n",
    "\n",
    "    # Create output directory if it doesn't exist\n",
    "    output_dir = \"results\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Generate output filename\n",
    "    base_filename = os.path.splitext(os.path.basename(file_path))[0]\n",
    "    output_file = os.path.join(output_dir, f\"{base_filename}_results.csv\")\n",
    "\n",
    "    # Save to CSV\n",
    "    df.to_csv(output_file, index=False)\n",
    "\n",
    "    print(f\"Results saved to {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
